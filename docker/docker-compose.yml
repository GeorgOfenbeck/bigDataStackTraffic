version: '3'

services:

  db:
   image: postgres
   restart: always
   ports:
      - 5432:5432
   environment:
      # POSTGRES_USER: user def postgres
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: accidents
   volumes:
      - local_pgdata:/var/lib/postgresql/data
  
  pgadmin:
   image: dpage/pgadmin4
   container_name: pgadmin4_container
   restart: always
   ports:
      - "5050:80"
   environment:
      PGADMIN_DEFAULT_EMAIL: pgadmin@mail.com
      PGADMIN_DEFAULT_PASSWORD: admin
   volumes:
      - pgadmin-data:/var/lib/pgadmin

  namenode:
    image: sbloodys/hadoop:3.3.6
    hostname: namenode
    container_name: namenode
    command: [ "hdfs", "namenode" ]
    ports:
      - 19870:9870
      - 8020:8020
    env_file:
      - docker-compose.config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    volumes:
      #      - hadoop_namenode:/hadoop/dfs/name
      - ./source_data/US_Accidents_March23.csv:/us_accidents.csv  #TODO:  Make sure to have source_data folder and data 
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://namenode:9870" ]
      interval: 5s
      timeout: 5s
      retries: 120
  datanode:
    image: sbloodys/hadoop:3.3.6
    hostname: datanode
    container_name: datanode
    command: [ "hdfs", "datanode" ]
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://datanode:9864" ]
      interval: 5s
      timeout: 5s
      retries: 120
    depends_on:
      namenode:
        condition: service_healthy
  resourcemanager:
    image: sbloodys/hadoop:3.3.6
    hostname: resourcemanager
    container_name: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 18088:8088
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://resourcemanager:8088" ]
      interval: 5s
      timeout: 5s
      retries: 120
  nodemanager:
    image: sbloodys/hadoop:3.3.6
    hostname: nodemanager
    container_name: nodemanager
    command: [ "yarn", "nodemanager" ]
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    depends_on:
      resourcemanager:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "http://nodemanager:8042" ]
      interval: 5s
      timeout: 5s
      retries: 120

  spark-master:
    image: ofenbeck/spark-master:3.5.4-hadoop3.4.1
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - PYSPARK_PYTHON=python3
    env_file:
      - ./hadoop.env
    volumes:
      - ../batch_processor:/spark/batch_processor

  spark-worker1:
    image: ofenbeck/spark-worker:3.5.4-hadoop3.4.1
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env
 
  spark-worker2:
    image: ofenbeck/spark-worker:3.5.4-hadoop3.4.1
    container_name: spark-worker2
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8082:8082
    env_file:
      - ./hadoop.env
      

volumes:
  # hadoop_namenode:
  # hadoop_datanode1:
  # hadoop_datanode2:
  local_pgdata:
  pgadmin-data:
  metabase-data:
  influxdb-data:
