version: '3'

services:

  namenode:
    image: sbloodys/hadoop:3.3.6
    hostname: namenode
    container_name: namenode
    command: [ "hdfs", "namenode" ]
    ports:
      - 19870:9870
    env_file:
      - docker-compose.config
    environment:
      ENSURE_NAMENODE_DIR: "/tmp/hadoop-root/dfs/name"
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://namenode:9870" ]
      interval: 5s
      timeout: 5s
      retries: 120
  datanode:
    image: sbloodys/hadoop:3.3.6
    hostname: datanode
    container_name: datanode
    command: [ "hdfs", "datanode" ]
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://datanode:9864" ]
      interval: 5s
      timeout: 5s
      retries: 120
    depends_on:
      namenode:
        condition: service_healthy
  resourcemanager:
    image: sbloodys/hadoop:3.3.6
    hostname: resourcemanager
    container_name: resourcemanager
    command: [ "yarn", "resourcemanager" ]
    ports:
      - 18088:8088
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    healthcheck:
      test: [ "CMD", "curl", "http://resourcemanager:8088" ]
      interval: 5s
      timeout: 5s
      retries: 120
  nodemanager:
    image: sbloodys/hadoop:3.3.6
    hostname: nodemanager
    container_name: nodemanager
    command: [ "yarn", "nodemanager" ]
    env_file:
      - docker-compose.config
    logging:
      driver: "json-file"
      options:
        max-size: "200m"
        max-file: "1"
    tty: true
    stdin_open: true
    restart: always
    depends_on:
      resourcemanager:
        condition: service_healthy
    healthcheck:
      test: [ "CMD", "curl", "http://nodemanager:8042" ]
      interval: 5s
      timeout: 5s
      retries: 120

  spark-master:
    image: ofenbeck/spark-master:3.5.4-hadoop3.4.1
    container_name: spark-master
    ports:
      - 8080:8080
      - 7077:7077
    environment:
      - PYSPARK_PYTHON=python3
    env_file:
      - ./hadoop.env
    volumes:
      - ../batch_processor:/spark/batch_processor

  spark-worker1:
    image: ofenbeck/spark-worker:3.5.4-hadoop3.4.1
    container_name: spark-worker1
    depends_on:
      - spark-master
    environment:
      - SPARK_MASTER=spark://spark-master:7077
    ports:
      - 8081:8081
    env_file:
      - ./hadoop.env
      

volumes:
  hadoop_namenode:
  hadoop_datanode1:
  hadoop_datanode2:
  local_pgdata:
  pgadmin-data:
  metabase-data:
  influxdb-data:
